{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd02647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data preparation\n",
    "Today we will use the 20newsgroups dataset as seen in the \"bitezize-NLP-prep-20newsgroups\" repository"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "stop_words = stopwords.words(\"english\") #load the stop words (words to ignore list) for english\n",
    "df = pd.DataFrame(pd.Series(fetch_20newsgroups(subset='train').data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment extraction\n",
    "\n",
    "def extractComments(x):\n",
    "    ''' INPUT: a string\n",
    "        OUTPUT: the right side of the string after splitting it\n",
    "            on the first double line break\n",
    "    '''\n",
    "    l = x.split('\\n\\n',1)\n",
    "    return l[1]\n",
    "\n",
    "df['comments'] = df[0].apply(lambda x: extractComments(x)).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment cleaning\n",
    "\n",
    "def scrubString(x):\n",
    "    ''' INPUT: a string\n",
    "        OUTPUT: a string that has had links removed, then non-letters, then english stopwords\n",
    "            This will produce a blank string if it only consisted of links, numbers, etc\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    x = re.sub(\"\\S*@\\S*\\s?\",\"\",x) #Remove email addresses\n",
    "    x = re.sub(\"#\\S+|&\\S+|@\\S+|https?:\\S+|RT|[^A-Za-z0-9]+\",' ', x) #Remove hyperlinks\n",
    "    x = re.sub(\"&\\S*|@\\S+|https?:\\S+\",' ', x) #Remove more hyperlinks\n",
    "    x = re.sub(\"[^A-Za-z']+\",' ',x) #keep only letters\n",
    "\n",
    "    if len(x)==0:\n",
    "        return ''\n",
    "    \n",
    "    tokens = word_tokenize(x) # Convert the string into tokens\n",
    "    \n",
    "    # Lemmatize the words, and only keep non-stop words\n",
    "    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]\n",
    "    \n",
    "    if len(tokens)==0:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(map(str,tokens))\n",
    "\n",
    "df['cleaned'] = df['comments'].apply(lambda x: scrubString(x))"
   ]
  },
  {
   "source": [
    "## Topic modelling\n",
    "Now we get to use gensim to extract topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save the tokens of the cleaned text\n",
    "\n",
    "def extract_tokens(x, min_len=5):\n",
    "    ''' INPUT: a string\n",
    "        OUTPUT: a list of tokens with that meet a minimum length\n",
    "    '''\n",
    "    tokens = word_tokenize(x)\n",
    "    tokens = [token for token in tokens if len(token) >= min_len]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['cleaned'].apply(lambda x: extract_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.033*\"entry\" + 0.014*\"section\" + 0.013*\"output\" + 0.013*\"program\" + 0.009*\"build\"')\n(1, '0.028*\"image\" + 0.020*\"window\" + 0.019*\"server\" + 0.016*\"program\" + 0.013*\"format\"')\n(2, '0.016*\"period\" + 0.014*\"militia\" + 0.010*\"Second\" + 0.009*\"Islanders\" + 0.008*\"Kings\"')\n(3, '0.034*\"writes\" + 0.031*\"article\" + 0.007*\"would\" + 0.007*\"think\" + 0.006*\"engine\"')\n(4, '0.017*\"player\" + 0.012*\"season\" + 0.010*\"writes\" + 0.009*\"think\" + 0.008*\"hockey\"')\n(5, '0.012*\"would\" + 0.012*\"drive\" + 0.012*\"problem\" + 0.009*\"Thanks\" + 0.009*\"system\"')\n(6, '0.015*\"device\" + 0.011*\"encryption\" + 0.011*\"RIPEM\" + 0.010*\"ground\" + 0.009*\"circuit\"')\n(7, '0.017*\"space\" + 0.010*\"Space\" + 0.009*\"launch\" + 0.008*\"orbit\" + 0.007*\"would\"')\n(8, '0.028*\"Jesus\" + 0.013*\"Christ\" + 0.009*\"church\" + 0.007*\"point\" + 0.007*\"Church\"')\n(9, '0.012*\"Gordon\" + 0.012*\"Master\" + 0.010*\"Banks\" + 0.009*\"cover\" + 0.008*\"article\"')\n(10, '0.015*\"Israel\" + 0.011*\"people\" + 0.010*\"Turkish\" + 0.010*\"Israeli\" + 0.009*\"Armenian\"')\n(11, '0.013*\"people\" + 0.013*\"would\" + 0.010*\"think\" + 0.010*\"writes\" + 0.008*\"believe\"')\n(12, '0.011*\"information\" + 0.007*\"available\" + 0.006*\"system\" + 0.006*\"University\" + 0.006*\"number\"')\n(13, '0.019*\"Islam\" + 0.015*\"Islamic\" + 0.008*\"Muslims\" + 0.008*\"libXmu\" + 0.007*\"Private\"')\n(14, '0.015*\"would\" + 0.011*\"people\" + 0.010*\"writes\" + 0.008*\"article\" + 0.007*\"right\"')\n"
     ]
    }
   ],
   "source": [
    "# Generate \n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "text_data = df['tokens'].to_list()\n",
    "\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model_15.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Generate visualizations for the topic model and save to an HTML file\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook(local=True)\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.save_html(vis,'lda_15.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}